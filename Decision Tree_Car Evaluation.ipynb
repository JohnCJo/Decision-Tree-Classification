{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9XvNv+itXUiJdESqSWHps"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QqPqx60DKGth"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from pprint import pprint"]},{"cell_type":"code","source":["#Load training data from local drive\n","from google.colab import files\n","uploaded = files.upload()\n","import io\n","car_data= pd.read_csv(io.BytesIO(uploaded['car_evaluation.csv']))\n","car_data.loc[253:255,:]"],"metadata":{"id":"jZdMAdGRWsy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["traindata=car_data.sample(frac=0.75, random_state=99)# Alternatively use the first 100 feature vectors for training without this random sampling as train=iris.loc[0:99]\n","testdata = car_data.loc[~car_data.index.isin(traindata.index), :] # and the remaining feature vectors for test test=iris.loc[100:149]\n","trainidx=np.arange(0,traindata.shape[0])\n","traindata.set_index(trainidx,inplace=True)\n","testidx=np.arange(0,testdata.shape[0])\n","testdata.set_index(testidx,inplace=True)\n","print(traindata.shape,testdata.shape)"],"metadata":{"id":"esetDZ67wPE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Form the dataset as X consisting of all training examples and features except the ground truths and Y consisiting of only ground truths of the\n","#corresponding training examples in X.\n","X=traindata.loc[:,\"price\":\"safety\"]\n","Y=traindata.loc[:,\"profitable\"]\n","print(X.head(3),\"\\n\", Y.head(3), X.shape)"],"metadata":{"id":"KzOBeF4Ev1Zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tdata=X.join(Y)\n","tdata.head(2)"],"metadata":{"id":"24fLnwNm0F8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Compute entropy\n","def entropy(fdata):\n","    values,counts = np.unique(fdata,return_counts=True)\n","    for i in range(len(values)):\n","        entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts))])\n","    return entropy"],"metadata":{"id":"ONVk1O39sAyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##Info Gain\n","\n","def InfoGain(X,Y,selected_feature):\n","    total_entropy = entropy(Y)\n","    vals,counts = np.unique(X[selected_feature],return_counts=True)\n","    for i in range(len(vals)):\n","        w=counts[i]/np.sum(counts)\n","        v=X.loc[(X[selected_feature]==vals[i]).dropna(),'profitable']\n","        Weighted_Entropy = np.sum([w*entropy(v)])           \n","    #formula for information gain\n","    Information_Gain = total_entropy-Weighted_Entropy\n","    return Information_Gain"],"metadata":{"id":"TtUnDnZAWtMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ID3(tdata,sdata,features,class_label=\"profitable\", parent_node_class=None):\n","    #tdata=X.join(Y)\n","    #If all class_label values are same, return that value\n","    if len(np.unique(sdata[class_label])) <= 1:\n","        return np.unique(sdata[class_label])[0]\n","    \n","    #if the dataset is empty or below some threshold value, terminate recursion\n","    elif len(sdata) <= 5:\n","        # Find the counts of distinct values of class_label, then find the maximum count of them--> majority class label\n","        return np.unique(tdata[class_label])[np.argmax(np.unique(tdata[class_label],\n","                                                                           return_counts=True)[1])]\n","    \n","    #If the feature space is empty, terminate recursion\n","    elif len(features) == 0:\n","        return parent_node_class \n","\n","    #If none of the above condition holds true form the subtrees\n","\n","    else:\n","        # Find the counts of distinct values of class_label, then find the maximum count of them--> majority class label\n","        parent_node_class = np.unique(sdata[class_label])[np.argmax(np.unique(sdata[class_label],\n","                                                                           return_counts=True)[1])]\n","\n","    #Select the feature which best splits the dataset, feature having maximum informatin gain\n","    for feature in features:\n","        item_values = [InfoGain(sdata,feature,class_label)] #Return the infgain values\n","    best_feature_index = np.argmax(item_values)\n","    best_feature = features[best_feature_index]\n","\n","    #Create the tree structure as a nested dictionary\n","    tree = {best_feature:{}}\n","\n","    #Remove the feature with the best info gain\n","    features = [i for i in features if i!= best_feature]\n","\n","    #Form subtrees down the root node by calling ID3 recursively\n","\n","    for value in np.unique(sdata[best_feature]):\n","        value = value\n","        sub_data = sdata.where(sdata[best_feature]==value).dropna()\n","        #call the ID3 algotirthm\n","        subtree = ID3(tdata,sub_data,features,class_label,parent_node_class)\n","        #Add the subtree\n","        tree[best_feature][value] = subtree\n","    return(tree)"],"metadata":{"id":"CfVeu4cyysvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tdata.columns[:]"],"metadata":{"id":"e0kqLhmG3vc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tree = ID3(tdata,tdata,tdata.columns[:-1])\n","pprint(tree)"],"metadata":{"id":"-QaFEAL_3i28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Predict the result\n","def predict(query,tree,default=1):\n","    for key in list(query.keys()):\n","        if key in list(tree.keys()):\n","            try:\n","               result = tree[key][query[key]]\n","               print(result)\n","            except:\n","               return default\n","\n","            result = tree[key][query[key]]\n","            if isinstance(result,dict):\n","                return predict(query,result)\n","            else:\n","                return result"],"metadata":{"id":"_NTsPrnx5ER3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(data,tree):\n","   queries = data.iloc[:,:-1].to_dict(orient=\"records\")\n","   predicted = pd.DataFrame(columns=[\"predicted\"])\n","\n","   #calculation of accuracy\n","\n","   for i in range(len(data)):\n","       predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0)\n","   print(\"The Prediction accuracy is:\",(np.sum(predicted[\"predicted\"]==data[\"profitable\"])/len(data))*100,'%')"],"metadata":{"id":"TpnHrmVP5Erm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train and print the tree, find the accuracy\n","test(testdata,tree)"],"metadata":{"id":"WMjEyoGw5Trn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Form the decision tree class by embedding the train and test methods and evaluate the classifier by forming confusion matrix"],"metadata":{"id":"Hn6NpHBCpQQ_"}},{"cell_type":"code","source":[],"metadata":{"id":"6BjUhHXWphcv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WIyFonFKphoi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Eo5NyLTWph0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Use scikit-learn library to form the decision tree classifier and evaluate its performance"],"metadata":{"id":"bRHLGD21pklh"}},{"cell_type":"code","source":[],"metadata":{"id":"-crgv4YdqF8u"},"execution_count":null,"outputs":[]}]}